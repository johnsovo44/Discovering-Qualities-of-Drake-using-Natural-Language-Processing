{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model---Rap-Genius\" data-toc-modified-id=\"Model---Rap-Genius-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model - Rap Genius</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-Modeling\" data-toc-modified-id=\"Topic-Modeling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Topic Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#All-Text\" data-toc-modified-id=\"All-Text-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>All Text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Findings\" data-toc-modified-id=\"Findings-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Findings</a></span></li></ul></li><li><span><a href=\"#Nouns-only\" data-toc-modified-id=\"Nouns-only-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Nouns only</a></span><ul class=\"toc-item\"><li><span><a href=\"#Findings\" data-toc-modified-id=\"Findings-1.1.2.1\"><span class=\"toc-item-num\">1.1.2.1&nbsp;&nbsp;</span>Findings</a></span></li></ul></li><li><span><a href=\"#Nouns-and-Adjectives\" data-toc-modified-id=\"Nouns-and-Adjectives-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Nouns and Adjectives</a></span></li></ul></li><li><span><a href=\"#Text-Generation\" data-toc-modified-id=\"Text-Generation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Text Generation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-a-Markov-Chains\" data-toc-modified-id=\"Build-a-Markov-Chains-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Build a Markov Chains</a></span></li><li><span><a href=\"#Create-a-text-generator\" data-toc-modified-id=\"Create-a-text-generator-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Create a text generator</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Rap Genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:33.114681Z",
     "start_time": "2019-05-21T16:19:27.735112Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import matutils, models # matutils will turn the array into a bag of words\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from IPython.lib.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "For topic modeling we are going to be using a technique called Laten Dirichlet Allocation (LDA). Using NLTK for part of speech tagging. We want to know what are the different themes that arise in a rappers lyrics, and who tends to talk about what.\n",
    "\n",
    "LDA deals with probability. Dirichlet is a type of probability and we are trying to discover what is the probability that the document is about a specific topic given a set of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Text\n",
    "The first use of LDA will be with all of the text as in I will not remove text based on the part of speech (verbs, nouns, adjectives included).  For this to work, instead of making a document term matrix we are going to make a term document matrix which is just the document term matrix transposed. The LDA takes in an array so in a way this creates an array per word when it is in this form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:33.182071Z",
     "start_time": "2019-05-21T16:19:33.128131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1008</th>\n",
       "      <th>10yearolds</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>125</th>\n",
       "      <th>140</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonin</th>\n",
       "      <th>zöld</th>\n",
       "      <th>ölén</th>\n",
       "      <th>úgy</th>\n",
       "      <th>な音楽</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 5270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           02  10  100  1000  1008  10yearolds  11  12  125  140  ...  zeros  \\\n",
       "Artist                                                            ...          \n",
       "Drake       0   0    6     0     0           0   0   0    0    0  ...      0   \n",
       "Jayz        0   0    2     0     0           0   2   0    0    1  ...      0   \n",
       "Nas         0   1    0     1     0           0   0   1    0    0  ...      1   \n",
       "Eminem      1   0    0     0     0           1   0   1    0    0  ...      0   \n",
       "Future      0   0    0     0     1           0   0   2    0    0  ...      0   \n",
       "KanyeWest   0   0    0     0     0           0   0   1    2    0  ...      0   \n",
       "\n",
       "           zip  zod  zombie  zone  zonin  zöld  ölén  úgy  な音楽  \n",
       "Artist                                                          \n",
       "Drake        1    0       0     1      0     0     0    0    0  \n",
       "Jayz         0    0       0     0      0     0     0    0    0  \n",
       "Nas          0    0       0     0      0     0     0    0    0  \n",
       "Eminem       0    1       1     0      0     0     0    0    0  \n",
       "Future       0    0       0     0      0     0     0    0    1  \n",
       "KanyeWest    0    0       1     0      3     1     1    1    0  \n",
       "\n",
       "[6 rows x 5270 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../Datasets/Pickled_Files/DataFrame_with_new_stopwords.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:36.208320Z",
     "start_time": "2019-05-21T16:19:36.189609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Artist</th>\n",
       "      <th>Drake</th>\n",
       "      <th>Jayz</th>\n",
       "      <th>Nas</th>\n",
       "      <th>Eminem</th>\n",
       "      <th>Future</th>\n",
       "      <th>KanyeWest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Artist  Drake  Jayz  Nas  Eminem  Future  KanyeWest\n",
       "02          0     0    0       1       0          0\n",
       "10          0     0    1       0       0          0\n",
       "100         6     2    0       0       0          0\n",
       "1000        0     0    1       0       0          0\n",
       "1008        0     0    0       0       1          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose to create a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:38.758029Z",
     "start_time": "2019-05-21T16:19:38.744721Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we are going to turn the tdm into a sparse matrix\n",
    "# where most of the elements are zero \n",
    "# (the opposite is a dense matrix where they are nonzero)\n",
    "\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:39.559848Z",
     "start_time": "2019-05-21T16:19:39.516426Z"
    }
   },
   "outputs": [],
   "source": [
    "# gensim requires a dictionary of all the terms and where they reside in the tdm\n",
    "# this will show us all the unique words in our corpus.\n",
    "cv = pickle.load(open('../Datasets/Pickled_Files/cv_stop.pkl', 'rb'))\n",
    "id2word = dict((v,k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we are going to look for 3 topics and do 100 passes. The more passes, the more accurate the model can get. Setting a random state to make the results reproducible. Alpha and Beta are two parameters needed to optimize the model. The alpha controls the mixture of topics for any given document. Turn it down and the documents will likely have less of a mixture of topics. Turn it up and the documents will likely have more of a mixture of topics. Beta is similar to alpha but instead it deals with terms for any given topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:16.049278Z",
     "start_time": "2019-05-17T00:25:26.081647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"just\" + 0.007*\"verse\" + 0.007*\"chorus\" + 0.007*\"bitch\" + 0.006*\"ass\" + 0.006*\"life\" + 0.005*\"need\" + 0.005*\"make\" + 0.005*\"real\" + 0.005*\"time\"'),\n",
       " (1,\n",
       "  '0.009*\"just\" + 0.005*\"say\" + 0.005*\"let\" + 0.005*\"em\" + 0.004*\"fuckin\" + 0.004*\"better\" + 0.004*\"think\" + 0.004*\"bitch\" + 0.004*\"yall\" + 0.004*\"verse\"'),\n",
       " (2,\n",
       "  '0.007*\"verse\" + 0.006*\"chorus\" + 0.005*\"just\" + 0.005*\"black\" + 0.005*\"bitch\" + 0.005*\"em\" + 0.005*\"west\" + 0.005*\"life\" + 0.004*\"kanye\" + 0.004*\"big\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA Model with 100 Passes\n",
    "lda = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=3,\n",
    "    passes=100,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', \n",
    "    eta = 'auto' \n",
    ")\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "Using all words we get the words Just, Just, and Verse as some of the top words that could be the topics for 0-2 (the topics are not given in LDA, only the probability of what it could be is given). The topics are not indicative of a subject matter that rappers would bring up. Focusing solely on the nouns may result in better topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:17.101796Z",
     "start_time": "2019-05-17T00:26:17.092480Z"
    }
   },
   "outputs": [],
   "source": [
    "def nouns(text):\n",
    "    \"\"\"\n",
    "    pull out the nouns only from a string of text\n",
    "    \"\"\"\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:29.962857Z",
     "start_time": "2019-05-17T00:26:29.919156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Artist Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>produced by boi1da frank dukes noah 40 shebib ...</td>\n",
       "      <td>Drake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams do i find it so hard whe...</td>\n",
       "      <td>Jayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>produced by ron browz intro fuck jay z whats u...</td>\n",
       "      <td>Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>verse 1 now this shits about to kick off this ...</td>\n",
       "      <td>Eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>intro high klassified な音楽 i got the truth in m...</td>\n",
       "      <td>Future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>produced by daft punk  kanye west verse 1 for ...</td>\n",
       "      <td>KanyeWest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics Artist Name\n",
       "Artist                                                                  \n",
       "Drake      produced by boi1da frank dukes noah 40 shebib ...       Drake\n",
       "Jayz       intro hannah williams do i find it so hard whe...        Jayz\n",
       "Nas        produced by ron browz intro fuck jay z whats u...         Nas\n",
       "Eminem     verse 1 now this shits about to kick off this ...      Eminem\n",
       "Future     intro high klassified な音楽 i got the truth in m...      Future\n",
       "KanyeWest  produced by daft punk  kanye west verse 1 for ...   KanyeWest"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the cleaned data to gather the nouns\n",
    "data_clean = pd.read_pickle('../Datasets/Pickled_Files/DataFrame_Corpus.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:47.371240Z",
     "start_time": "2019-05-17T00:26:41.476506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>boi1da frank dukes shebib part verse fuck bein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams i heart im day day look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>ron browz fuck jay z ayo i z dick nigga style ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>party lets hiphop scratch im bout track everyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>な音楽 i truth weeknd dont dance moves nigga sham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>daft punk kanye verse theme song jeans byanyme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics\n",
       "Artist                                                      \n",
       "Drake      boi1da frank dukes shebib part verse fuck bein...\n",
       "Jayz       intro hannah williams i heart im day day look ...\n",
       "Nas        ron browz fuck jay z ayo i z dick nigga style ...\n",
       "Eminem     party lets hiphop scratch im bout track everyb...\n",
       "Future     な音楽 i truth weeknd dont dance moves nigga sham...\n",
       "KanyeWest  daft punk kanye verse theme song jeans byanyme..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the noun function that was created above\n",
    "data_nouns = pd.DataFrame(data_clean.Lyrics.apply(nouns))\n",
    "data_nouns\n",
    "# below you will see the corpus with nouns only. It seems like some\n",
    "# words just don't belong like the word 'moves' but notice the word dance next\n",
    "# to it. That makes it a noun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps that we had done in the all text version of the model have to be done again. Meaning we another term document matrix and another dictionary before modeling. Also the term document matrix does not have stop words in it. Re-adding them in and considering other words that shouldn't be in the corpus but will not be part of the stop words initially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:50.319690Z",
     "start_time": "2019-05-17T00:26:50.235632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>ability</th>\n",
       "      <th>abundance</th>\n",
       "      <th>accelerants</th>\n",
       "      <th>accolades</th>\n",
       "      <th>account</th>\n",
       "      <th>accounts</th>\n",
       "      <th>ace</th>\n",
       "      <th>acetaminophen</th>\n",
       "      <th>...</th>\n",
       "      <th>yung</th>\n",
       "      <th>zapatos</th>\n",
       "      <th>zazie</th>\n",
       "      <th>ze</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 3328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a1  aaaah  ability  abundance  accelerants  accolades  account  \\\n",
       "Artist                                                                      \n",
       "Drake       0      0        0          0            0          0        0   \n",
       "Jayz        0      0        0          0            0          0        0   \n",
       "Nas         0      0        0          1            0          0        0   \n",
       "Eminem      0      0        1          0            1          1        1   \n",
       "Future      0      0        0          0            0          0        1   \n",
       "KanyeWest   1      1        0          0            0          0        0   \n",
       "\n",
       "           accounts  ace  acetaminophen  ...  yung  zapatos  zazie  ze  zeros  \\\n",
       "Artist                                   ...                                    \n",
       "Drake             0    0              0  ...     0        0      1   0      0   \n",
       "Jayz              0    0              0  ...     0        1      0   0      0   \n",
       "Nas               1    1              0  ...     0        0      0   1      1   \n",
       "Eminem            0    1              1  ...     1        0      0   0      0   \n",
       "Future            0    1              0  ...     0        0      0   0      0   \n",
       "KanyeWest         1    0              0  ...     0        0      0   0      0   \n",
       "\n",
       "           zip  zod  zombie  zone  zonin  \n",
       "Artist                                    \n",
       "Drake        1    0       0     1      0  \n",
       "Jayz         0    0       0     0      0  \n",
       "Nas          0    0       0     0      0  \n",
       "Eminem       0    1       1     0      0  \n",
       "Future       0    0       0     0      0  \n",
       "KanyeWest    0    0       1     0      3  \n",
       "\n",
       "[6 rows x 3328 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of added stop words\n",
    "add_stop_words = [ \n",
    "    'な音楽','verse','produced','intro','just','em','chorus',\n",
    "    'bitch','kanye','west','boi1da','ass','yall', 'zöld',\n",
    "    'ölén','úgy', 'im',\n",
    "    \n",
    "    'fuck','fucking','fucks','fuckin','nigga','niggas','shit', \n",
    "    'bitch','bitches','pussy','hoes','muhfucka','motherfucker',\n",
    "    'ass', 'dont', 'ya','yuh','ayy','ta','ive'\n",
    "    \n",
    "    'lets','cause','thats', 'youre','aint', 'yeah', 'future','nas',\n",
    "    'drake'\n",
    "]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Create a document term matrix to turn into a term document matrix\n",
    "cvn = CountVectorizer(\n",
    "    stop_words=stop_words\n",
    ")\n",
    "data_cvn = cvn.fit_transform(\n",
    "    data_nouns.Lyrics\n",
    ")\n",
    "data_dtmn = pd.DataFrame(\n",
    "    data_cvn.toarray(),\n",
    "    columns = cvn.get_feature_names()\n",
    ")\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:57.935606Z",
     "start_time": "2019-05-17T00:26:57.919543Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we are going to turn the tdm into a sparse matrix\n",
    "# where most of the elements are zero \n",
    "# (the opposite is a dense matrix where they are nonzero)\n",
    "\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "id2wordn = dict((v,k) for k,v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:42.208458Z",
     "start_time": "2019-05-17T00:27:29.161592Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"love\" + 0.018*\"world\" + 0.015*\"man\" + 0.012*\"time\" + 0.007*\"way\" + 0.007*\"represent\" + 0.007*\"life\" + 0.006*\"face\" + 0.006*\"home\" + 0.005*\"things\"'),\n",
       " (1,\n",
       "  '0.000*\"world\" + 0.000*\"love\" + 0.000*\"man\" + 0.000*\"life\" + 0.000*\"time\" + 0.000*\"day\" + 0.000*\"clique\" + 0.000*\"gon\" + 0.000*\"money\" + 0.000*\"represent\"'),\n",
       " (2,\n",
       "  '0.020*\"clique\" + 0.012*\"swerve\" + 0.007*\"girl\" + 0.007*\"money\" + 0.007*\"hands\" + 0.006*\"monster\" + 0.006*\"life\" + 0.005*\"teeth\" + 0.005*\"sound\" + 0.005*\"need\"'),\n",
       " (3,\n",
       "  '0.010*\"life\" + 0.009*\"day\" + 0.008*\"lets\" + 0.007*\"time\" + 0.007*\"thou\" + 0.006*\"money\" + 0.006*\"trophy\" + 0.005*\"gon\" + 0.005*\"dog\" + 0.005*\"commas\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying 4 topics w/ 50 passes\n",
    "ldan = models.LdaModel(\n",
    "    corpus=corpusn,\n",
    "    id2word=id2wordn,\n",
    "    num_topics=4,\n",
    "    passes=50,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', \n",
    "    eta = 'auto' \n",
    ")\n",
    "\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "The words that came out of this model are good, but not great. Still below 10% probabilities but now we have better ideas of what the topics may be.\n",
    "- Topic 0: Love\n",
    "- Topic 1: World\n",
    "- Topic 2: Clique (Group, friends)\n",
    "- Topic 3: Life\n",
    "\n",
    "The topics sound better but what is concerning is the low probability. Taking a look at Topic 1 all of the probabilities are 0 making it difficult to understand what the real topic is.\n",
    "\n",
    "Another thing is is it seems like the topics are repeating. The interpretation of the output is usually subjective and the first few words are taken into account to understand the true topic. But I am noticing the same nouns are showing up multiple times making it difficult to make a subjective topic discovery, instead I am just going off the first word I see. If I were to add in the adjectives would this change the how the topics are interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:43.904741Z",
     "start_time": "2019-05-17T00:27:43.897720Z"
    }
   },
   "outputs": [],
   "source": [
    "def noun_adj(text):\n",
    "    \"\"\"\n",
    "    Same as nouns above, but now including adjectives too.\n",
    "    \"\"\"\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word,pos) in pos_tag(tokenized) if is_noun_adj (pos)]\n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:51.196026Z",
     "start_time": "2019-05-17T00:27:45.558411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>boi1da frank dukes shebib nineteen85 part vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams hard i heart im day day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>ron browz intro fuck jay z niggas ayo i aint j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>verse party wack lets hiphop scratch im bout t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>high な音楽 i truth verse weeknd nigga dont dance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>daft punk kanye west verse theme song black le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics\n",
       "Artist                                                      \n",
       "Drake      boi1da frank dukes shebib nineteen85 part vers...\n",
       "Jayz       intro hannah williams hard i heart im day day ...\n",
       "Nas        ron browz intro fuck jay z niggas ayo i aint j...\n",
       "Eminem     verse party wack lets hiphop scratch im bout t...\n",
       "Future     high な音楽 i truth verse weeknd nigga dont dance...\n",
       "KanyeWest  daft punk kanye west verse theme song black le..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the noun_adj function\n",
    "data_nouns_adj = pd.DataFrame(data_clean.Lyrics.apply(noun_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:53.000004Z",
     "start_time": "2019-05-17T00:27:52.914757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21st</th>\n",
       "      <th>41st</th>\n",
       "      <th>a1</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absurd</th>\n",
       "      <th>abundance</th>\n",
       "      <th>ac</th>\n",
       "      <th>accelerants</th>\n",
       "      <th>...</th>\n",
       "      <th>yung</th>\n",
       "      <th>zapatos</th>\n",
       "      <th>zazie</th>\n",
       "      <th>ze</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 4041 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           21st  41st  a1  aaaah  ability  able  absurd  abundance  ac  \\\n",
       "Artist                                                                   \n",
       "Drake         0     0   0      0        0     0       0          0   0   \n",
       "Jayz          1     0   0      0        0     0       0          0   0   \n",
       "Nas           0     1   0      0        0     0       0          1   0   \n",
       "Eminem        0     0   0      0        1     2       1          0   1   \n",
       "Future        0     0   0      0        0     0       0          0   0   \n",
       "KanyeWest     0     0   1      1        0     0       0          0   0   \n",
       "\n",
       "           accelerants  ...  yung  zapatos  zazie  ze  zeros  zip  zod  \\\n",
       "Artist                  ...                                              \n",
       "Drake                0  ...     0        0      1   0      0    1    0   \n",
       "Jayz                 0  ...     0        1      0   0      0    0    0   \n",
       "Nas                  0  ...     0        0      0   1      1    0    0   \n",
       "Eminem               1  ...     1        0      0   0      0    0    1   \n",
       "Future               0  ...     0        0      0   0      0    0    0   \n",
       "KanyeWest            0  ...     0        0      0   0      0    0    0   \n",
       "\n",
       "           zombie  zone  zonin  \n",
       "Artist                          \n",
       "Drake           0     1      0  \n",
       "Jayz            0     0      0  \n",
       "Nas             0     0      0  \n",
       "Eminem          1     0      0  \n",
       "Future          0     0      0  \n",
       "KanyeWest       1     0      3  \n",
       "\n",
       "[6 rows x 4041 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate countvectorizer\n",
    "cvna = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    ")\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.Lyrics)\n",
    "data_dtmna = pd.DataFrame (data_cvna.toarray(), columns = cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:55.922524Z",
     "start_time": "2019-05-17T00:27:55.910279Z"
    }
   },
   "outputs": [],
   "source": [
    "# create our sparse matrix and vocab dictionary\n",
    "corpusna = matutils.Sparse2Corpus(\n",
    "    scipy.sparse.csr_matrix(data_dtmna.transpose())\n",
    ")\n",
    "\n",
    "id2wordna = dict((v,k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:34:02.090736Z",
     "start_time": "2019-05-17T00:28:24.651359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"life\" + 0.010*\"high\" + 0.010*\"lets\" + 0.009*\"thou\" + 0.009*\"low\" + 0.008*\"day\" + 0.007*\"money\" + 0.007*\"trophy\" + 0.006*\"commas\" + 0.006*\"dog\"'),\n",
       " (1,\n",
       "  '0.006*\"better\" + 0.005*\"gon\" + 0.005*\"common\" + 0.005*\"time\" + 0.005*\"moment\" + 0.005*\"oohoohoohooh\" + 0.004*\"eminem\" + 0.004*\"hes\" + 0.004*\"day\" + 0.004*\"way\"'),\n",
       " (2,\n",
       "  '0.018*\"love\" + 0.014*\"world\" + 0.011*\"man\" + 0.010*\"time\" + 0.008*\"real\" + 0.006*\"way\" + 0.006*\"represent\" + 0.005*\"black\" + 0.005*\"life\" + 0.005*\"new\"'),\n",
       " (3,\n",
       "  '0.016*\"clique\" + 0.009*\"god\" + 0.009*\"swerve\" + 0.007*\"black\" + 0.007*\"big\" + 0.006*\"girl\" + 0.005*\"monster\" + 0.005*\"money\" + 0.005*\"hands\" + 0.005*\"sean\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics with 1000 passes\n",
    "ldana = models.LdaModel(\n",
    "    corpus=corpusna,\n",
    "    id2word=id2wordna,\n",
    "    num_topics=4,\n",
    "    passes=1000,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', \n",
    "    eta = 'auto' \n",
    ")\n",
    "\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:28.258714Z",
     "start_time": "2019-05-17T01:02:28.242662Z"
    }
   },
   "outputs": [],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(ldana.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in ldana.show_topic(t, topn = 5)])\n",
    "\n",
    "display = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:29.330663Z",
     "start_time": "2019-05-17T01:02:29.310665Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Word</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>life</td>\n",
       "      <td>0.011404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>0.009815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>lets</td>\n",
       "      <td>0.009588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.008680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>thou</td>\n",
       "      <td>0.008680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>better</td>\n",
       "      <td>0.005964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>gon</td>\n",
       "      <td>0.005428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>common</td>\n",
       "      <td>0.005160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>time</td>\n",
       "      <td>0.004892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>moment</td>\n",
       "      <td>0.004892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>0.018148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>world</td>\n",
       "      <td>0.014176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>man</td>\n",
       "      <td>0.011308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>time</td>\n",
       "      <td>0.009543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>real</td>\n",
       "      <td>0.007998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>clique</td>\n",
       "      <td>0.015914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>god</td>\n",
       "      <td>0.009451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>swerve</td>\n",
       "      <td>0.009451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>black</td>\n",
       "      <td>0.007189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>big</td>\n",
       "      <td>0.006543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic    Word         P\n",
       "0       0    life  0.011404\n",
       "1       0    high  0.009815\n",
       "2       0    lets  0.009588\n",
       "3       0     low  0.008680\n",
       "4       0    thou  0.008680\n",
       "5       1  better  0.005964\n",
       "6       1     gon  0.005428\n",
       "7       1  common  0.005160\n",
       "8       1    time  0.004892\n",
       "9       1  moment  0.004892\n",
       "10      2    love  0.018148\n",
       "11      2   world  0.014176\n",
       "12      2     man  0.011308\n",
       "13      2    time  0.009543\n",
       "14      2    real  0.007998\n",
       "15      3  clique  0.015914\n",
       "16      3     god  0.009451\n",
       "17      3  swerve  0.009451\n",
       "18      3   black  0.007189\n",
       "19      3     big  0.006543"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks alot better. The topics are really diverse with the adjectives added in. The probabilities are better, unlike the noun model. Even if the nouns repeat the adjectives set it apart. Interpretation is not too difficult. Seems like Topic 0 is about Life and Drugs, Topic 1 is difficult to interpret so I will go with 'A Better Life', Topic 2 love and the world, Topic 3 sounds like Kanye who is a topic unto himself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:30.363617Z",
     "start_time": "2019-05-17T01:02:30.343654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'Drake'),\n",
       " (0, 'Jayz'),\n",
       " (2, 'Nas'),\n",
       " (1, 'Eminem'),\n",
       " (0, 'Future'),\n",
       " (3, 'KanyeWest')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we wanted to know what topic does each artist usually discuss or what topic is most relevant to each artist. The information of this is hidden at first but once extracted we can see that Drake and Nas are yet again in parallel residing in topic 2 (love and the world, or maybe their world). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "Using a corpus we can use markov chains to generate brand new text. We preserve the order of the text and punctuation too. We can generate new text based off this corpus in the style of a certain rapper, like Drake. This is just a demonstration of markov chains.\n",
    "\n",
    "Markov Chains are a mathematical way of representing how systems change over time. Unfortunately they are memoryless and only knows what happens in one previous state. Basically, tomorrow's weather is based on what happens today. \n",
    "\n",
    "Put another way Today it rained, there is a 50% chance it will rain again, 30% chance it will be sunny and, 20% chance it will be cloudy. The system will choose rainy, but now there is a 30% chance it will rain, 50% chance it will be sunny, and 20% chance it will be cloudy. Now the system chooses sunny and the system starts over again.\n",
    "\n",
    "For the computer to do this we can create a dictionary, the key being a word, and the value a list of words that would proceed that word. We then will randomly select a word from that list that appears a number of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:24:40.402384Z",
     "start_time": "2019-05-21T16:24:40.350628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Artist Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>produced by boi1da frank dukes noah 40 shebib ...</td>\n",
       "      <td>Drake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams do i find it so hard whe...</td>\n",
       "      <td>Jayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>produced by ron browz intro fuck jay z whats u...</td>\n",
       "      <td>Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>verse 1 now this shits about to kick off this ...</td>\n",
       "      <td>Eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>intro high klassified な音楽 i got the truth in m...</td>\n",
       "      <td>Future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>produced by daft punk  kanye west verse 1 for ...</td>\n",
       "      <td>KanyeWest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics Artist Name\n",
       "Artist                                                                  \n",
       "Drake      produced by boi1da frank dukes noah 40 shebib ...       Drake\n",
       "Jayz       intro hannah williams do i find it so hard whe...        Jayz\n",
       "Nas        produced by ron browz intro fuck jay z whats u...         Nas\n",
       "Eminem     verse 1 now this shits about to kick off this ...      Eminem\n",
       "Future     intro high klassified な音楽 i got the truth in m...      Future\n",
       "KanyeWest  produced by daft punk  kanye west verse 1 for ...   KanyeWest"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data, the corpus\n",
    "data = pd.read_pickle('../Datasets/Pickled_Files/DataFrame_Corpus.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Markov Chains\n",
    "- Keys are all the words in the corpus \n",
    "- Values are all the words that follow the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:24:43.267054Z",
     "start_time": "2019-05-21T16:24:43.249015Z"
    }
   },
   "outputs": [],
   "source": [
    "def markov_chain(text):\n",
    "    \n",
    "    #tokenize the text by word, including punctuation\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # initialize the dictionary that will hold all the keys and values\n",
    "    m_dict = defaultdict(list)\n",
    "    \n",
    "    # create a list of the word: list of words format\n",
    "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
    "        m_dict[current_word].append(next_word)\n",
    "    \n",
    "    # convert it back to a dictionary\n",
    "    m_dict = dict(m_dict)\n",
    "    return m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:25:13.247916Z",
     "start_time": "2019-05-21T16:25:13.207999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'produced': ['by', 'by'],\n",
      " 'by': ['boi1da', 'key', 'the', 'a', 'nineteen85', ...],\n",
      " 'boi1da': ['frank', 'whats'],\n",
      " 'frank': ['dukes'],\n",
      " 'dukes': ['noah'],\n",
      " ...}\n"
     ]
    }
   ],
   "source": [
    "drake_dictionary = markov_chain(drake_text)\n",
    "pprint(drake_dictionary,max_seq_length= 5)\n",
    "# there are alot of dupilicate. That's okay because that means there is a\n",
    "# high probability that word will get picked after the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:51:33.550969Z",
     "start_time": "2019-05-17T13:51:33.529282Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sentence (chain, count=20):\n",
    "    \n",
    "    # capitalize the first word\n",
    "    word1 = random.choice(list(chain.keys()))\n",
    "    sentence = word1.capitalize()\n",
    "    \n",
    "    # generate the second word from the value list. \n",
    "    # don't forget to use this new word as word 1 to move forward.\n",
    "    for i in range(count-1):\n",
    "        word2 = random.choice(chain[word1])\n",
    "        word1 = word2\n",
    "        sentence += ' ' + word2\n",
    "        \n",
    "    # end it with a period\n",
    "    sentence += '...'\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:57:11.182830Z",
     "start_time": "2019-05-17T13:57:11.166596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Outlive me all me stay up im the shine i know it like eight advances god damn okay made yall...'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(drake_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:00:27.190898Z",
     "start_time": "2019-05-16T05:00:27.184449Z"
    },
    "collapsed": true
   },
   "source": [
    "**Drake Samples**\n",
    "\n",
    "'Perspective i did you cause youre talkin boasy and shit 0 to do it i dont even know it you.'\n",
    "\n",
    "\n",
    "'All me no more time where where i see i been cookin with my cell phone late night late night.'\n",
    "\n",
    "\n",
    "'Cuddle with the goats for someone else you left your advances god damn chorus gods plan i had a lot...'\n",
    "\n",
    "\n",
    "'Million off and when she say youll never ever leave from beside me cause i was a good girl and...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:25:55.540424Z",
     "start_time": "2019-05-21T16:25:55.515902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'produced': ['by'],\n",
      " 'by': ['ron', 'rashad', 'large', 'aesop', 'villanova', ...],\n",
      " 'ron': ['browz'],\n",
      " 'browz': ['intro'],\n",
      " 'intro': ['fuck', 'nas', 'az', 'yeah', 'umm'],\n",
      " ...}\n"
     ]
    }
   ],
   "source": [
    "nasDictionary =  markov_chain(nasText)\n",
    "pprint(nasDictionary,max_seq_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:56:49.643682Z",
     "start_time": "2019-05-17T13:56:49.634418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changes and ill defeat foes yall rock fellas put you traded your mothafuckin mind i aint the story yesterday when...'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(nasDictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nas Samples**\n",
    "\n",
    "'Headed for breakfast dime sexes and i had me laugh in these last time is i fuck i trap em.'\n",
    "\n",
    "\n",
    "'Prisoner set free all races combined in hand in a maximum state pen and be riffin while im a cheetah...'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
