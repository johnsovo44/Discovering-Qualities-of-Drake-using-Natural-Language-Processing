{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model---Rap-Genius\" data-toc-modified-id=\"Model---Rap-Genius-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model - Rap Genius</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic-Modeling\" data-toc-modified-id=\"Topic-Modeling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Topic Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Attempt-1----All-Text\" data-toc-modified-id=\"Attempt-1----All-Text-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Attempt 1 -- All Text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adjust-the-topics-count\" data-toc-modified-id=\"Adjust-the-topics-count-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Adjust the topics count</a></span></li><li><span><a href=\"#Adjust-the-number-of-passes\" data-toc-modified-id=\"Adjust-the-number-of-passes-1.1.1.2\"><span class=\"toc-item-num\">1.1.1.2&nbsp;&nbsp;</span>Adjust the number of passes</a></span></li></ul></li><li><span><a href=\"#Nouns-only\" data-toc-modified-id=\"Nouns-only-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Nouns only</a></span></li><li><span><a href=\"#Nouns-and-Adjectives\" data-toc-modified-id=\"Nouns-and-Adjectives-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Nouns and Adjectives</a></span></li></ul></li><li><span><a href=\"#Text-Generation\" data-toc-modified-id=\"Text-Generation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Text Generation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-data-to-imitate\" data-toc-modified-id=\"Read-in-data-to-imitate-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Read in data to imitate</a></span></li><li><span><a href=\"#Build-a-Markov-Chains\" data-toc-modified-id=\"Build-a-Markov-Chains-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Build a Markov Chains</a></span></li><li><span><a href=\"#Create-a-text-generator\" data-toc-modified-id=\"Create-a-text-generator-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Create a text generator</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Rap Genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:33.114681Z",
     "start_time": "2019-05-21T16:19:27.735112Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import matutils, models # matutils will turn the array into a bag of words\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from IPython.lib.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "for topic modeling we are going to be using a technique called Laten Dirichlet Allocation (LDA). NLTK for part of speech tagging. We want to know what are the different themes that arise in a rappers lyrics, and who tends to talk about what.\n",
    "\n",
    "LDA deals with probability. Dirichlet is a type of probability and we are trying to discover what is the probability that the document is about a specific topic given a set of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 -- All Text\n",
    "This is not going to work the first time through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:33.182071Z",
     "start_time": "2019-05-21T16:19:33.128131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1008</th>\n",
       "      <th>10yearolds</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>125</th>\n",
       "      <th>140</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonin</th>\n",
       "      <th>zöld</th>\n",
       "      <th>ölén</th>\n",
       "      <th>úgy</th>\n",
       "      <th>な音楽</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 5270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           02  10  100  1000  1008  10yearolds  11  12  125  140  ...  zeros  \\\n",
       "Artist                                                            ...          \n",
       "Drake       0   0    6     0     0           0   0   0    0    0  ...      0   \n",
       "Jayz        0   0    2     0     0           0   2   0    0    1  ...      0   \n",
       "Nas         0   1    0     1     0           0   0   1    0    0  ...      1   \n",
       "Eminem      1   0    0     0     0           1   0   1    0    0  ...      0   \n",
       "Future      0   0    0     0     1           0   0   2    0    0  ...      0   \n",
       "KanyeWest   0   0    0     0     0           0   0   1    2    0  ...      0   \n",
       "\n",
       "           zip  zod  zombie  zone  zonin  zöld  ölén  úgy  な音楽  \n",
       "Artist                                                          \n",
       "Drake        1    0       0     1      0     0     0    0    0  \n",
       "Jayz         0    0       0     0      0     0     0    0    0  \n",
       "Nas          0    0       0     0      0     0     0    0    0  \n",
       "Eminem       0    1       1     0      0     0     0    0    0  \n",
       "Future       0    0       0     0      0     0     0    0    1  \n",
       "KanyeWest    0    0       1     0      3     1     1    1    0  \n",
       "\n",
       "[6 rows x 5270 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../Datasets/Pickled_Files/DataFrame_with_new_stopwords.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:36.208320Z",
     "start_time": "2019-05-21T16:19:36.189609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Artist</th>\n",
       "      <th>Drake</th>\n",
       "      <th>Jayz</th>\n",
       "      <th>Nas</th>\n",
       "      <th>Eminem</th>\n",
       "      <th>Future</th>\n",
       "      <th>KanyeWest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Artist  Drake  Jayz  Nas  Eminem  Future  KanyeWest\n",
       "02          0     0    0       1       0          0\n",
       "10          0     0    1       0       0          0\n",
       "100         6     2    0       0       0          0\n",
       "1000        0     0    1       0       0          0\n",
       "1008        0     0    0       0       1          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose to create a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:38.758029Z",
     "start_time": "2019-05-21T16:19:38.744721Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we are going to turn the tdm into a sparse matrix\n",
    "# where most of the elements are zero (the opposite is a dense matrix where they are nonzero)\n",
    "\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:19:39.559848Z",
     "start_time": "2019-05-21T16:19:39.516426Z"
    }
   },
   "outputs": [],
   "source": [
    "# gensim requires a dictionary of all the terms and where they reside in the tdm\n",
    "# this will show us all the unique words in our corpus.\n",
    "cv = pickle.load(open('../Datasets/Pickled_Files/cv_stop.pkl', 'rb'))\n",
    "id2word = dict((v,k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:24:36.383941Z",
     "start_time": "2019-05-17T00:24:31.903007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"verse\" + 0.007*\"life\" + 0.006*\"bitch\" + 0.006*\"chorus\" + 0.005*\"just\" + 0.005*\"ass\" + 0.004*\"high\" + 0.004*\"new\" + 0.004*\"em\" + 0.004*\"time\"'),\n",
       " (1,\n",
       "  '0.010*\"just\" + 0.006*\"need\" + 0.006*\"verse\" + 0.006*\"chorus\" + 0.005*\"want\" + 0.005*\"say\" + 0.004*\"bitch\" + 0.004*\"time\" + 0.004*\"make\" + 0.004*\"em\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the LDA model\n",
    "\n",
    "lda = models.LdaModel(\n",
    "    corpus=corpus, # this our term document matrix\n",
    "    id2word=id2word, # this our dict of location:term\n",
    "    num_topics=2, # choosing two topics the model will try to discover\n",
    "    passes=10, # we will start with 10 passes and see what difference it makes moving up or down.\n",
    "              # this will go through the document once searching for the best topics based off the document. I'm asking it to do it 10x.\n",
    "              # the more passes the more the topics begin to make sense.\n",
    "    random_state = 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "# print the discovered topics out\n",
    "lda.print_topics()\n",
    "# the output will show you the top words of the topic. It will not output the topic itself.\n",
    "# this output probably won't make much sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust the topics count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:24:53.834300Z",
     "start_time": "2019-05-17T00:24:48.476692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"just\" + 0.007*\"verse\" + 0.007*\"chorus\" + 0.007*\"bitch\" + 0.006*\"ass\" + 0.006*\"life\" + 0.005*\"need\" + 0.005*\"make\" + 0.005*\"real\" + 0.005*\"time\"'),\n",
       " (1,\n",
       "  '0.009*\"just\" + 0.005*\"say\" + 0.005*\"let\" + 0.005*\"em\" + 0.004*\"fuckin\" + 0.004*\"think\" + 0.004*\"better\" + 0.004*\"bitch\" + 0.004*\"verse\" + 0.004*\"yall\"'),\n",
       " (2,\n",
       "  '0.007*\"verse\" + 0.006*\"chorus\" + 0.005*\"just\" + 0.005*\"black\" + 0.005*\"bitch\" + 0.005*\"life\" + 0.005*\"em\" + 0.005*\"west\" + 0.004*\"kanye\" + 0.004*\"want\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try 3 topics\n",
    "lda = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=3,\n",
    "    passes=10,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "lda.print_topics()\n",
    "# not getting anything better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:24:59.883255Z",
     "start_time": "2019-05-17T00:24:54.961359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"just\" + 0.008*\"verse\" + 0.007*\"chorus\" + 0.007*\"bitch\" + 0.007*\"ass\" + 0.006*\"life\" + 0.005*\"need\" + 0.005*\"make\" + 0.005*\"real\" + 0.005*\"time\"'),\n",
       " (1,\n",
       "  '0.010*\"just\" + 0.005*\"let\" + 0.005*\"say\" + 0.005*\"em\" + 0.005*\"fuckin\" + 0.005*\"think\" + 0.005*\"better\" + 0.004*\"bitch\" + 0.004*\"verse\" + 0.004*\"yall\"'),\n",
       " (2,\n",
       "  '0.007*\"verse\" + 0.006*\"chorus\" + 0.006*\"just\" + 0.005*\"black\" + 0.005*\"bitch\" + 0.005*\"life\" + 0.005*\"em\" + 0.005*\"west\" + 0.005*\"kanye\" + 0.004*\"want\"'),\n",
       " (3,\n",
       "  '0.000*\"just\" + 0.000*\"verse\" + 0.000*\"time\" + 0.000*\"bitch\" + 0.000*\"life\" + 0.000*\"chorus\" + 0.000*\"ass\" + 0.000*\"make\" + 0.000*\"need\" + 0.000*\"em\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try 4\n",
    "# let's try 3 topics\n",
    "lda = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=4,\n",
    "    passes=10,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "lda.print_topics()\n",
    "# not getting anything better...again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to need to go back and clean much more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust the number of passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:25:24.986024Z",
     "start_time": "2019-05-17T00:25:00.927634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"just\" + 0.007*\"verse\" + 0.007*\"chorus\" + 0.007*\"bitch\" + 0.006*\"ass\" + 0.006*\"life\" + 0.005*\"need\" + 0.005*\"real\" + 0.005*\"make\" + 0.005*\"high\"'),\n",
       " (1,\n",
       "  '0.009*\"just\" + 0.005*\"let\" + 0.005*\"say\" + 0.005*\"em\" + 0.004*\"fuckin\" + 0.004*\"better\" + 0.004*\"think\" + 0.004*\"bitch\" + 0.004*\"yall\" + 0.004*\"verse\"'),\n",
       " (2,\n",
       "  '0.007*\"verse\" + 0.006*\"chorus\" + 0.005*\"just\" + 0.005*\"black\" + 0.005*\"bitch\" + 0.005*\"west\" + 0.005*\"em\" + 0.005*\"life\" + 0.004*\"kanye\" + 0.004*\"big\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll use 4 topics w/ 50 passes\n",
    "lda = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=3,\n",
    "    passes=50,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "lda.print_topics()\n",
    "# that didn't seem to work any better. One last shot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:16.049278Z",
     "start_time": "2019-05-17T00:25:26.081647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"just\" + 0.007*\"verse\" + 0.007*\"chorus\" + 0.007*\"bitch\" + 0.006*\"ass\" + 0.006*\"life\" + 0.005*\"need\" + 0.005*\"make\" + 0.005*\"real\" + 0.005*\"time\"'),\n",
       " (1,\n",
       "  '0.009*\"just\" + 0.005*\"say\" + 0.005*\"let\" + 0.005*\"em\" + 0.004*\"fuckin\" + 0.004*\"better\" + 0.004*\"think\" + 0.004*\"bitch\" + 0.004*\"yall\" + 0.004*\"verse\"'),\n",
       " (2,\n",
       "  '0.007*\"verse\" + 0.006*\"chorus\" + 0.005*\"just\" + 0.005*\"black\" + 0.005*\"bitch\" + 0.005*\"em\" + 0.005*\"west\" + 0.005*\"life\" + 0.004*\"kanye\" + 0.004*\"big\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll try w/ 100 passes\n",
    "lda = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=3,\n",
    "    passes=100,\n",
    "#     distributed=True, # This will not work without a module named Pyro4\n",
    "    random_state= 42,\n",
    "    per_word_topics=False, # prints the list of topics (not working)\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "lda.print_topics()\n",
    "# still the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:17.101796Z",
     "start_time": "2019-05-17T00:26:17.092480Z"
    }
   },
   "outputs": [],
   "source": [
    "def nouns(text):\n",
    "    \"\"\"\n",
    "    pull out the nouns only from a string of text\n",
    "    \"\"\"\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:29.962857Z",
     "start_time": "2019-05-17T00:26:29.919156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Artist Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>produced by boi1da frank dukes noah 40 shebib ...</td>\n",
       "      <td>Drake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams do i find it so hard whe...</td>\n",
       "      <td>Jayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>produced by ron browz intro fuck jay z whats u...</td>\n",
       "      <td>Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>verse 1 now this shits about to kick off this ...</td>\n",
       "      <td>Eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>intro high klassified な音楽 i got the truth in m...</td>\n",
       "      <td>Future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>produced by daft punk  kanye west verse 1 for ...</td>\n",
       "      <td>KanyeWest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics Artist Name\n",
       "Artist                                                                  \n",
       "Drake      produced by boi1da frank dukes noah 40 shebib ...       Drake\n",
       "Jayz       intro hannah williams do i find it so hard whe...        Jayz\n",
       "Nas        produced by ron browz intro fuck jay z whats u...         Nas\n",
       "Eminem     verse 1 now this shits about to kick off this ...      Eminem\n",
       "Future     intro high klassified な音楽 i got the truth in m...      Future\n",
       "KanyeWest  produced by daft punk  kanye west verse 1 for ...   KanyeWest"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the cleaned data to gather the nouns\n",
    "data_clean = pd.read_pickle('../Datasets/Pickled_Files/DataFrame_Corpus.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:47.371240Z",
     "start_time": "2019-05-17T00:26:41.476506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>boi1da frank dukes shebib part verse fuck bein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams i heart im day day look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>ron browz fuck jay z ayo i z dick nigga style ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>party lets hiphop scratch im bout track everyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>な音楽 i truth weeknd dont dance moves nigga sham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>daft punk kanye verse theme song jeans byanyme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics\n",
       "Artist                                                      \n",
       "Drake      boi1da frank dukes shebib part verse fuck bein...\n",
       "Jayz       intro hannah williams i heart im day day look ...\n",
       "Nas        ron browz fuck jay z ayo i z dick nigga style ...\n",
       "Eminem     party lets hiphop scratch im bout track everyb...\n",
       "Future     な音楽 i truth weeknd dont dance moves nigga sham...\n",
       "KanyeWest  daft punk kanye verse theme song jeans byanyme..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the noun function that was created above\n",
    "data_nouns = pd.DataFrame(data_clean.Lyrics.apply(nouns))\n",
    "data_nouns\n",
    "# below you will see the corpus with nouns only. It seems like some\n",
    "# words just don't belong like the word 'moves' but notice the word dance next\n",
    "# to it. That makes it a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:50.319690Z",
     "start_time": "2019-05-17T00:26:50.235632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>ability</th>\n",
       "      <th>abundance</th>\n",
       "      <th>accelerants</th>\n",
       "      <th>accolades</th>\n",
       "      <th>account</th>\n",
       "      <th>accounts</th>\n",
       "      <th>ace</th>\n",
       "      <th>acetaminophen</th>\n",
       "      <th>...</th>\n",
       "      <th>yung</th>\n",
       "      <th>zapatos</th>\n",
       "      <th>zazie</th>\n",
       "      <th>ze</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 3328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a1  aaaah  ability  abundance  accelerants  accolades  account  \\\n",
       "Artist                                                                      \n",
       "Drake       0      0        0          0            0          0        0   \n",
       "Jayz        0      0        0          0            0          0        0   \n",
       "Nas         0      0        0          1            0          0        0   \n",
       "Eminem      0      0        1          0            1          1        1   \n",
       "Future      0      0        0          0            0          0        1   \n",
       "KanyeWest   1      1        0          0            0          0        0   \n",
       "\n",
       "           accounts  ace  acetaminophen  ...  yung  zapatos  zazie  ze  zeros  \\\n",
       "Artist                                   ...                                    \n",
       "Drake             0    0              0  ...     0        0      1   0      0   \n",
       "Jayz              0    0              0  ...     0        1      0   0      0   \n",
       "Nas               1    1              0  ...     0        0      0   1      1   \n",
       "Eminem            0    1              1  ...     1        0      0   0      0   \n",
       "Future            0    1              0  ...     0        0      0   0      0   \n",
       "KanyeWest         1    0              0  ...     0        0      0   0      0   \n",
       "\n",
       "           zip  zod  zombie  zone  zonin  \n",
       "Artist                                    \n",
       "Drake        1    0       0     1      0  \n",
       "Jayz         0    0       0     0      0  \n",
       "Nas          0    0       0     0      0  \n",
       "Eminem       0    1       1     0      0  \n",
       "Future       0    0       0     0      0  \n",
       "KanyeWest    0    0       1     0      3  \n",
       "\n",
       "[6 rows x 3328 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay now we have to do the same steps above all over again\n",
    "# meaning we need another term document matrix and another dictionary \n",
    "# before modeling. \n",
    "\n",
    "# right now our tdm doesn not have stop words in it. Re-add them\n",
    "add_stop_words = [ # inside this list I will insert words that just shouldn't be considered\n",
    "    'な音楽','verse','produced','intro','just','em','chorus',\n",
    "    'bitch','kanye','west','boi1da','ass','yall', 'zöld',\n",
    "    'ölén','úgy', 'im',\n",
    "    \n",
    "    'fuck','fucking','fucks','fuckin','nigga','niggas','shit', \n",
    "    'bitch','bitches','pussy','hoes','muhfucka','motherfucker',\n",
    "    'ass', 'dont', 'ya','yuh','ayy','ta','ive'\n",
    "    \n",
    "    'lets','cause','thats', 'youre','aint', 'yeah', 'future','nas',\n",
    "    'drake'\n",
    "]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Create a document term matrix to turn into a term document matrix\n",
    "cvn = CountVectorizer(\n",
    "    stop_words=stop_words\n",
    ")\n",
    "data_cvn = cvn.fit_transform(\n",
    "    data_nouns.Lyrics\n",
    ")\n",
    "data_dtmn = pd.DataFrame(\n",
    "    data_cvn.toarray(),\n",
    "    columns = cvn.get_feature_names()\n",
    ")\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:26:57.935606Z",
     "start_time": "2019-05-17T00:26:57.919543Z"
    }
   },
   "outputs": [],
   "source": [
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "id2wordn = dict((v,k) for k,v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:11.842377Z",
     "start_time": "2019-05-17T00:27:08.890629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"love\" + 0.012*\"world\" + 0.010*\"man\" + 0.008*\"time\" + 0.006*\"life\" + 0.006*\"york\" + 0.006*\"day\" + 0.005*\"way\" + 0.005*\"baby\" + 0.004*\"represent\"'),\n",
       " (1,\n",
       "  '0.009*\"life\" + 0.007*\"clique\" + 0.007*\"time\" + 0.006*\"lets\" + 0.006*\"money\" + 0.006*\"thou\" + 0.005*\"gon\" + 0.005*\"trophy\" + 0.005*\"everybody\" + 0.004*\"day\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the LDA model\n",
    "\n",
    "ldan = models.LdaModel(\n",
    "    corpus=corpusn, # this our term document matrix\n",
    "    id2word=id2wordn, # this our dict of location:term\n",
    "    num_topics=2, # choosing two topics the model will try to discover\n",
    "    passes=10, # we will start with 10 passes and see what difference it makes moving up or down.\n",
    "              # this will go through the document once searching for the best topics based off the document. I'm asking it to do it 10x.\n",
    "              # the more passes the more the topics begin to make sense.\n",
    "    random_state = 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "# print the discovered topics out\n",
    "ldan.print_topics()\n",
    "# the output will show you the top words of the topic. It will not output the topic itself.\n",
    "# this is starting look decent as an output do to the stop words, \n",
    "# which only reinforces that I need to clean better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:27.558170Z",
     "start_time": "2019-05-17T00:27:13.500092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"love\" + 0.013*\"world\" + 0.011*\"man\" + 0.009*\"time\" + 0.007*\"life\" + 0.007*\"york\" + 0.006*\"day\" + 0.005*\"way\" + 0.005*\"baby\" + 0.005*\"represent\"'),\n",
       " (1,\n",
       "  '0.010*\"clique\" + 0.006*\"swerve\" + 0.006*\"time\" + 0.005*\"gon\" + 0.005*\"everybody\" + 0.005*\"life\" + 0.004*\"day\" + 0.004*\"moment\" + 0.004*\"monster\" + 0.004*\"man\"'),\n",
       " (2,\n",
       "  '0.016*\"life\" + 0.016*\"thou\" + 0.015*\"lets\" + 0.012*\"trophy\" + 0.011*\"dog\" + 0.011*\"commas\" + 0.009*\"money\" + 0.008*\"percocets\" + 0.007*\"time\" + 0.006*\"wifey\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying 3 topics w/ 50 passes\n",
    "ldan = models.LdaModel(\n",
    "    corpus=corpusn,\n",
    "    id2word=id2wordn,\n",
    "    num_topics=3,\n",
    "    passes=50,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:42.208458Z",
     "start_time": "2019-05-17T00:27:29.161592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"love\" + 0.018*\"world\" + 0.015*\"man\" + 0.012*\"time\" + 0.007*\"way\" + 0.007*\"represent\" + 0.007*\"life\" + 0.006*\"face\" + 0.006*\"home\" + 0.005*\"things\"'),\n",
       " (1,\n",
       "  '0.000*\"world\" + 0.000*\"love\" + 0.000*\"man\" + 0.000*\"life\" + 0.000*\"time\" + 0.000*\"day\" + 0.000*\"clique\" + 0.000*\"gon\" + 0.000*\"money\" + 0.000*\"represent\"'),\n",
       " (2,\n",
       "  '0.020*\"clique\" + 0.012*\"swerve\" + 0.007*\"girl\" + 0.007*\"money\" + 0.007*\"hands\" + 0.006*\"monster\" + 0.006*\"life\" + 0.005*\"teeth\" + 0.005*\"sound\" + 0.005*\"need\"'),\n",
       " (3,\n",
       "  '0.010*\"life\" + 0.009*\"day\" + 0.008*\"lets\" + 0.007*\"time\" + 0.007*\"thou\" + 0.006*\"money\" + 0.006*\"trophy\" + 0.005*\"gon\" + 0.005*\"dog\" + 0.005*\"commas\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying 4 topics w/ 50 passes\n",
    "ldan = models.LdaModel(\n",
    "    corpus=corpusn,\n",
    "    id2word=id2wordn,\n",
    "    num_topics=4,\n",
    "    passes=50,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:43.904741Z",
     "start_time": "2019-05-17T00:27:43.897720Z"
    }
   },
   "outputs": [],
   "source": [
    "def noun_adj(text):\n",
    "    \"\"\"\n",
    "    Same as nouns above, but now including adjectives too.\n",
    "    \"\"\"\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word,pos) in pos_tag(tokenized) if is_noun_adj (pos)]\n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:51.196026Z",
     "start_time": "2019-05-17T00:27:45.558411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>boi1da frank dukes shebib nineteen85 part vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams hard i heart im day day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>ron browz intro fuck jay z niggas ayo i aint j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>verse party wack lets hiphop scratch im bout t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>high な音楽 i truth verse weeknd nigga dont dance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>daft punk kanye west verse theme song black le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics\n",
       "Artist                                                      \n",
       "Drake      boi1da frank dukes shebib nineteen85 part vers...\n",
       "Jayz       intro hannah williams hard i heart im day day ...\n",
       "Nas        ron browz intro fuck jay z niggas ayo i aint j...\n",
       "Eminem     verse party wack lets hiphop scratch im bout t...\n",
       "Future     high な音楽 i truth verse weeknd nigga dont dance...\n",
       "KanyeWest  daft punk kanye west verse theme song black le..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(data_clean.Lyrics.apply(noun_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:53.000004Z",
     "start_time": "2019-05-17T00:27:52.914757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21st</th>\n",
       "      <th>41st</th>\n",
       "      <th>a1</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absurd</th>\n",
       "      <th>abundance</th>\n",
       "      <th>ac</th>\n",
       "      <th>accelerants</th>\n",
       "      <th>...</th>\n",
       "      <th>yung</th>\n",
       "      <th>zapatos</th>\n",
       "      <th>zazie</th>\n",
       "      <th>ze</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 4041 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           21st  41st  a1  aaaah  ability  able  absurd  abundance  ac  \\\n",
       "Artist                                                                   \n",
       "Drake         0     0   0      0        0     0       0          0   0   \n",
       "Jayz          1     0   0      0        0     0       0          0   0   \n",
       "Nas           0     1   0      0        0     0       0          1   0   \n",
       "Eminem        0     0   0      0        1     2       1          0   1   \n",
       "Future        0     0   0      0        0     0       0          0   0   \n",
       "KanyeWest     0     0   1      1        0     0       0          0   0   \n",
       "\n",
       "           accelerants  ...  yung  zapatos  zazie  ze  zeros  zip  zod  \\\n",
       "Artist                  ...                                              \n",
       "Drake                0  ...     0        0      1   0      0    1    0   \n",
       "Jayz                 0  ...     0        1      0   0      0    0    0   \n",
       "Nas                  0  ...     0        0      0   1      1    0    0   \n",
       "Eminem               1  ...     1        0      0   0      0    0    1   \n",
       "Future               0  ...     0        0      0   0      0    0    0   \n",
       "KanyeWest            0  ...     0        0      0   0      0    0    0   \n",
       "\n",
       "           zombie  zone  zonin  \n",
       "Artist                          \n",
       "Drake           0     1      0  \n",
       "Jayz            0     0      0  \n",
       "Nas             0     0      0  \n",
       "Eminem          1     0      0  \n",
       "Future          0     0      0  \n",
       "KanyeWest       1     0      3  \n",
       "\n",
       "[6 rows x 4041 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate countvectorizer\n",
    "cvna = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "#     max_df=.8 # consider adding this in, see what difference it makes\n",
    ")\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.Lyrics)\n",
    "data_dtmna = pd.DataFrame (data_cvna.toarray(), columns = cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:27:55.922524Z",
     "start_time": "2019-05-17T00:27:55.910279Z"
    }
   },
   "outputs": [],
   "source": [
    "# create our sparse matrix and vocab dictionary\n",
    "corpusna = matutils.Sparse2Corpus(\n",
    "    scipy.sparse.csr_matrix(data_dtmna.transpose())\n",
    ")\n",
    "\n",
    "id2wordna = dict((v,k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:28:01.962473Z",
     "start_time": "2019-05-17T00:27:58.477577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"life\" + 0.006*\"high\" + 0.006*\"time\" + 0.006*\"money\" + 0.006*\"real\" + 0.006*\"girl\" + 0.006*\"clique\" + 0.006*\"lets\" + 0.005*\"god\" + 0.005*\"big\"'),\n",
       " (1,\n",
       "  '0.010*\"world\" + 0.010*\"love\" + 0.006*\"man\" + 0.005*\"time\" + 0.004*\"life\" + 0.004*\"gon\" + 0.004*\"represent\" + 0.004*\"better\" + 0.003*\"new\" + 0.003*\"yo\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to model. We will start with 2 and move to 4\n",
    "\n",
    "ldana = models.LdaModel(\n",
    "    corpus=corpusna,\n",
    "    id2word=id2wordna,\n",
    "    num_topics=2,\n",
    "    passes=10,\n",
    "    random_state = 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:28:22.188436Z",
     "start_time": "2019-05-17T00:28:04.508583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"life\" + 0.009*\"high\" + 0.009*\"lets\" + 0.008*\"low\" + 0.008*\"thou\" + 0.007*\"day\" + 0.007*\"money\" + 0.006*\"trophy\" + 0.006*\"dog\" + 0.006*\"new\"'),\n",
       " (1,\n",
       "  '0.005*\"better\" + 0.005*\"gon\" + 0.005*\"common\" + 0.005*\"time\" + 0.005*\"moment\" + 0.004*\"oohoohoohooh\" + 0.004*\"hes\" + 0.004*\"eminem\" + 0.004*\"day\" + 0.004*\"way\"'),\n",
       " (2,\n",
       "  '0.013*\"love\" + 0.009*\"world\" + 0.009*\"man\" + 0.008*\"clique\" + 0.008*\"time\" + 0.007*\"black\" + 0.006*\"real\" + 0.006*\"life\" + 0.005*\"big\" + 0.005*\"god\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 topics with 50 passes\n",
    "ldana = models.LdaModel(\n",
    "    corpus=corpusna,\n",
    "    id2word=id2wordna,\n",
    "    num_topics=3,\n",
    "    passes=50,\n",
    "    random_state = 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T00:34:02.090736Z",
     "start_time": "2019-05-17T00:28:24.651359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"life\" + 0.010*\"high\" + 0.010*\"lets\" + 0.009*\"thou\" + 0.009*\"low\" + 0.008*\"day\" + 0.007*\"money\" + 0.007*\"trophy\" + 0.006*\"commas\" + 0.006*\"dog\"'),\n",
       " (1,\n",
       "  '0.006*\"better\" + 0.005*\"gon\" + 0.005*\"common\" + 0.005*\"time\" + 0.005*\"moment\" + 0.005*\"oohoohoohooh\" + 0.004*\"eminem\" + 0.004*\"hes\" + 0.004*\"day\" + 0.004*\"way\"'),\n",
       " (2,\n",
       "  '0.018*\"love\" + 0.014*\"world\" + 0.011*\"man\" + 0.010*\"time\" + 0.008*\"real\" + 0.006*\"way\" + 0.006*\"represent\" + 0.005*\"black\" + 0.005*\"life\" + 0.005*\"new\"'),\n",
       " (3,\n",
       "  '0.016*\"clique\" + 0.009*\"god\" + 0.009*\"swerve\" + 0.007*\"black\" + 0.007*\"big\" + 0.006*\"girl\" + 0.005*\"monster\" + 0.005*\"money\" + 0.005*\"hands\" + 0.005*\"sean\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 topics with 1000 passes\n",
    "ldana = models.LdaModel(\n",
    "    corpus=corpusna,\n",
    "    id2word=id2wordna,\n",
    "    num_topics=4,\n",
    "    passes=1000,\n",
    "    random_state= 42,\n",
    "    alpha = 'auto', # The alpha controls the mixture of topics for any given document. \n",
    "                   # Turn it down and the documents will likely have less of a mixture of topics. \n",
    "                   # Turn it up and the documents will likely have more of a mixture of topics.\n",
    "    eta = 'auto' # this is beta. This is just like alpha but instead it deals with words for any given topic.\n",
    ")\n",
    "\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You are going to have to interpret the topics. Very subjective\n",
    "- Another technique you can apply is once you have ran your documents through LDA you can do a DBSCAN to do a more strict categorization of the documents. \n",
    "- LDA is a fuzzy categorization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:26.848416Z",
     "start_time": "2019-05-17T01:02:26.840453Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['life', 'high', 'lets', 'thou', 'low', 'day', 'money', 'trophy', 'dog', 'commas', 'new', 'big', 'time', 'percocets', 'york', 'young', 'bad', 'strong', 'real', 'baby', 'water', 'uh', 'problems', 'wifey', 'mask', 'house', 'girl', 'metro', 'gang', 'foreigns']\n",
      "Topic: 1 \n",
      "Words: ['better', 'gon', 'common', 'moment', 'time', 'oohoohoohooh', 'eminem', 'hes', 'day', 'way', 'man', 'rap', 'music', 'crazy', 'everybody', 'little', 'ive', 'ill', 'head', 'shot', 'yo', 'boy', 'mic', 'joyner', 'people', 'woo', 'lil', 'world', 'night', 'ready']\n",
      "Topic: 2 \n",
      "Words: ['love', 'world', 'man', 'time', 'real', 'represent', 'way', 'black', 'life', 'new', 'face', 'fake', 'home', 'girl', 'things', 'night', 'big', 'mind', 'minewhose', 'wishin', 'son', 'quick', 'york', 'thing', 'baby', 'yo', 'death', 'days', 'long', 'state']\n",
      "Topic: 3 \n",
      "Words: ['clique', 'god', 'swerve', 'black', 'big', 'girl', 'hands', 'monster', 'money', 'life', 'sean', 'mornin', 'need', 'thirsty', 'teeth', 'new', 'sound', 'high', 'weepin', 'choir', 'everybody', 'gnashin', 'chick', 'jerk', 'iiiiim', 'twoseat', 'motherfucking', 'dream', 'na', 'jesus']\n"
     ]
    }
   ],
   "source": [
    "for index, topic in ldana.show_topics(formatted=False, num_words= 30):\n",
    "        print('Topic: {} \\nWords: {}'.format(index, [w[0] for w in topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:28.258714Z",
     "start_time": "2019-05-17T01:02:28.242662Z"
    }
   },
   "outputs": [],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(ldana.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in ldana.show_topic(t, topn = 5)])\n",
    "\n",
    "display = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:29.330663Z",
     "start_time": "2019-05-17T01:02:29.310665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Word</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>life</td>\n",
       "      <td>0.011404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>0.009815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>lets</td>\n",
       "      <td>0.009588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.008680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>thou</td>\n",
       "      <td>0.008680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>better</td>\n",
       "      <td>0.005964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>gon</td>\n",
       "      <td>0.005428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>common</td>\n",
       "      <td>0.005160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>time</td>\n",
       "      <td>0.004892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>moment</td>\n",
       "      <td>0.004892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>0.018148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>world</td>\n",
       "      <td>0.014176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>man</td>\n",
       "      <td>0.011308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>time</td>\n",
       "      <td>0.009543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>real</td>\n",
       "      <td>0.007998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>clique</td>\n",
       "      <td>0.015914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>god</td>\n",
       "      <td>0.009451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>swerve</td>\n",
       "      <td>0.009451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>black</td>\n",
       "      <td>0.007189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>big</td>\n",
       "      <td>0.006543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic    Word         P\n",
       "0       0    life  0.011404\n",
       "1       0    high  0.009815\n",
       "2       0    lets  0.009588\n",
       "3       0     low  0.008680\n",
       "4       0    thou  0.008680\n",
       "5       1  better  0.005964\n",
       "6       1     gon  0.005428\n",
       "7       1  common  0.005160\n",
       "8       1    time  0.004892\n",
       "9       1  moment  0.004892\n",
       "10      2    love  0.018148\n",
       "11      2   world  0.014176\n",
       "12      2     man  0.011308\n",
       "13      2    time  0.009543\n",
       "14      2    real  0.007998\n",
       "15      3  clique  0.015914\n",
       "16      3     god  0.009451\n",
       "17      3  swerve  0.009451\n",
       "18      3   black  0.007189\n",
       "19      3     big  0.006543"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T01:02:30.363617Z",
     "start_time": "2019-05-17T01:02:30.343654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'Drake'),\n",
       " (0, 'Jayz'),\n",
       " (2, 'Nas'),\n",
       " (1, 'Eminem'),\n",
       " (0, 'Future'),\n",
       " (3, 'KanyeWest')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "Using a corpus we can use markov chains to generate brand new text. We preserve the order of the text and punctuation too. We can generate new text based off this corpus in the style of a certain rapper, like Drake.\n",
    "\n",
    "Markov Chains are a mathematical way of representing how systems change over time. Unfortunately they are memoryless and only knows what happens in one previous state. Basically, tomorrow's weather is based on what happens today. \n",
    "\n",
    "Put another way Today it rained, there is a 50% chance it will rain again, 30% chance it will be sunny and, 20% chance it will be cloudy. The system will choose rainy, but now there is a 30% chance it will rain, 50% chance it will be sunny, and 20% chance it will be cloudy. Now the system chooses sunny and the system starts over again.\n",
    "\n",
    "For the computer to do this we can create a dictionary, the key being a word, and the value a list of words that would proceed that word. We then will randomly select a word from that list that appears a number of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data to imitate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:24:40.402384Z",
     "start_time": "2019-05-21T16:24:40.350628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Artist Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artist</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drake</th>\n",
       "      <td>produced by boi1da frank dukes noah 40 shebib ...</td>\n",
       "      <td>Drake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jayz</th>\n",
       "      <td>intro hannah williams do i find it so hard whe...</td>\n",
       "      <td>Jayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nas</th>\n",
       "      <td>produced by ron browz intro fuck jay z whats u...</td>\n",
       "      <td>Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eminem</th>\n",
       "      <td>verse 1 now this shits about to kick off this ...</td>\n",
       "      <td>Eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Future</th>\n",
       "      <td>intro high klassified な音楽 i got the truth in m...</td>\n",
       "      <td>Future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KanyeWest</th>\n",
       "      <td>produced by daft punk  kanye west verse 1 for ...</td>\n",
       "      <td>KanyeWest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Lyrics Artist Name\n",
       "Artist                                                                  \n",
       "Drake      produced by boi1da frank dukes noah 40 shebib ...       Drake\n",
       "Jayz       intro hannah williams do i find it so hard whe...        Jayz\n",
       "Nas        produced by ron browz intro fuck jay z whats u...         Nas\n",
       "Eminem     verse 1 now this shits about to kick off this ...      Eminem\n",
       "Future     intro high klassified な音楽 i got the truth in m...      Future\n",
       "KanyeWest  produced by daft punk  kanye west verse 1 for ...   KanyeWest"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../Datasets/Pickled_Files/DataFrame_Corpus.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:24:41.565827Z",
     "start_time": "2019-05-21T16:24:41.548868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'produced by boi1da frank dukes noah 40 shebib  nineteen85 part i 0 to 100 verse 1 fuck bein on some chill shit we go 0 to 100 nigga real quick they be on that raptopaythebill shit and i dont feel that'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drake_text = data.Lyrics.loc['Drake']\n",
    "drake_text[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Markov Chains\n",
    "- Keys are all the words in the corpus \n",
    "- Values are all the words that follow the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:24:43.267054Z",
     "start_time": "2019-05-21T16:24:43.249015Z"
    }
   },
   "outputs": [],
   "source": [
    "def markov_chain(text):\n",
    "    \n",
    "    #tokenize the text by word, including punctuation\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # initialize the dictionary that will hold all the keys and values\n",
    "    m_dict = defaultdict(list)\n",
    "    \n",
    "    # create a list of the word: list of words format\n",
    "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
    "        m_dict[current_word].append(next_word)\n",
    "    \n",
    "    # convert it back to a dictionary\n",
    "    m_dict = dict(m_dict)\n",
    "    return m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:25:13.247916Z",
     "start_time": "2019-05-21T16:25:13.207999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'produced': ['by', 'by'],\n",
      " 'by': ['boi1da', 'key', 'the', 'a', 'nineteen85', ...],\n",
      " 'boi1da': ['frank', 'whats'],\n",
      " 'frank': ['dukes'],\n",
      " 'dukes': ['noah'],\n",
      " ...}\n"
     ]
    }
   ],
   "source": [
    "drake_dictionary = markov_chain(drake_text)\n",
    "pprint(drake_dictionary,max_seq_length= 5)\n",
    "# there are alot of dupes. That's okay because that means there is a\n",
    "# high probability that word will get picked after the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:51:33.550969Z",
     "start_time": "2019-05-17T13:51:33.529282Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sentence (chain, count=20):\n",
    "    \n",
    "    # capitalize the first word\n",
    "    word1 = random.choice(list(chain.keys()))\n",
    "    sentence = word1.capitalize()\n",
    "    \n",
    "    # generate the second word from the value list. \n",
    "    # don't forget to use this new word as word 1 to move forward.\n",
    "    for i in range(count-1):\n",
    "        word2 = random.choice(chain[word1])\n",
    "        word1 = word2\n",
    "        sentence += ' ' + word2\n",
    "        \n",
    "    # end it with a period\n",
    "    # psuedo code for later: if final word is not an determiner/article, then:\n",
    "    sentence += '...'\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:57:11.182830Z",
     "start_time": "2019-05-17T13:57:11.166596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Outlive me all me stay up im the shine i know it like eight advances god damn okay made yall...'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(drake_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:00:27.190898Z",
     "start_time": "2019-05-16T05:00:27.184449Z"
    },
    "collapsed": true
   },
   "source": [
    "**Drake Samples**\n",
    "\n",
    "'Perspective i did you cause youre talkin boasy and shit 0 to do it i dont even know it you.'\n",
    "\n",
    "\n",
    "'All me no more time where where i see i been cookin with my cell phone late night late night.'\n",
    "\n",
    "\n",
    "'Cuddle with the goats for someone else you left your advances god damn chorus gods plan i had a lot...'\n",
    "\n",
    "\n",
    "'Million off and when she say youll never ever leave from beside me cause i was a good girl and...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:25:35.343327Z",
     "start_time": "2019-05-21T16:25:35.329527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'produced by ron browz intro fuck jay z whats up niggas ayo i know you aint talkin about me dog you what fuck jay z you been on my dick nigga you love my style nigga fuck jay z chorus i fuck with your '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasText = data.Lyrics.loc['Nas']\n",
    "nasText[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:25:55.540424Z",
     "start_time": "2019-05-21T16:25:55.515902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'produced': ['by'],\n",
      " 'by': ['ron', 'rashad', 'large', 'aesop', 'villanova', ...],\n",
      " 'ron': ['browz'],\n",
      " 'browz': ['intro'],\n",
      " 'intro': ['fuck', 'nas', 'az', 'yeah', 'umm'],\n",
      " ...}\n"
     ]
    }
   ],
   "source": [
    "nasDictionary =  markov_chain(nasText)\n",
    "pprint(nasDictionary,max_seq_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:56:49.643682Z",
     "start_time": "2019-05-17T13:56:49.634418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changes and ill defeat foes yall rock fellas put you traded your mothafuckin mind i aint the story yesterday when...'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(nasDictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nas Samples**\n",
    "\n",
    "'Headed for breakfast dime sexes and i had me laugh in these last time is i fuck i trap em.'\n",
    "\n",
    "\n",
    "'Prisoner set free all races combined in hand in a maximum state pen and be riffin while im a cheetah...'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
